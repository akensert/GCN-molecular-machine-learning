{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from mlp import datasets as mlp_datasets\n",
    "from mlp import models as mlp_models\n",
    "\n",
    "from utils import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model_obj,\n",
    "                    model_params,\n",
    "                    model_weights,\n",
    "                    save_path):\n",
    "\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    print(best_params)\n",
    "    \n",
    "    batch_size = model_params['batch_size']\n",
    "    num_epochs = model_params['num_epochs']\n",
    "    bits = model_params['bits']\n",
    "    radius = model_params['radius']\n",
    "    use_counts = model_params['use_counts']\n",
    "\n",
    "    del model_params['batch_size']\n",
    "    del model_params['num_epochs']\n",
    "    del model_params['bits']\n",
    "    del model_params['radius']\n",
    "    del model_params['use_counts']\n",
    "    \n",
    "    train, valid, test_1, test_2 = mlp_datasets.get_ecfp_datasets(\n",
    "        f\"../input/datasets/{save_path.split('/')[-2]}.csv\",\n",
    "        bits=bits, radius=radius, use_counts=use_counts,\n",
    "    )\n",
    "    \n",
    "    for name, dataset in zip(['train', 'valid', 'test_1', 'test_2'], [train, valid, test_1, test_2]):\n",
    "        \n",
    "        if dataset is not None:\n",
    "            model = model_obj(**model_params)\n",
    "\n",
    "            model(train['X'][:1])\n",
    "            model.set_weights(model_weights)\n",
    "            \n",
    "            y_pred = model.predict(dataset['X'], dataset['y'])[1]\n",
    "\n",
    "            np.save(save_path + '/' + name, np.stack([dataset['y'], y_pred]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'num_layers':    lambda:       np.random.randint(*[1, 3+1]),\n",
    "    'num_units':     lambda:       np.random.randint(*[256, 1024+1]),\n",
    "    'learning_rate': lambda:       np.random.uniform(*[1e-4, 1e-3]),\n",
    "    'num_epochs':    lambda:       np.random.randint(*[50, 200+1]),\n",
    "    'batch_size':    lambda:       np.random.choice([32, 64, 128]),\n",
    "    'dropout_rate':  lambda:       np.random.uniform(*[0.0, 0.3]),\n",
    "    'bits':          lambda:       np.random.randint(*[512, 2048+1]),\n",
    "    'radius':        lambda:       np.random.randint(*[1, 3+1]),\n",
    "    'use_counts':    lambda:       np.random.choice([True, False]),\n",
    "}\n",
    "\n",
    "model_name = 'mlp'\n",
    "\n",
    "dataset_names = ['RIKEN', 'Fiehn_HILIC', 'SMRT'] # list(configuration.datasets.keys())\n",
    "\n",
    "NUM_SEARCHES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7811136976877847\n",
      "0.8921930249532062\n",
      "0.6078309163069113\n",
      "0.7071530941205145\n",
      "0.796317323782505\n",
      "0.6611532186850523\n",
      "0.6005431714424719\n",
      "0.6156541971059946\n",
      "0.9457356717036319\n",
      "0.6313683228615002\n",
      "0.6761680451417579\n",
      "0.5424267450968424\n",
      "0.5323121275045932\n",
      "0.5530084920540833\n",
      "0.8333074294603787\n",
      "0.6600142734478681\n",
      "0.5801115621664584\n",
      "0.5153267366458208\n",
      "0.709316985790546\n",
      "0.6746473009158402\n",
      "{'num_epochs': 107, 'batch_size': 64, 'hidden_units': [684, 684], 'initial_learning_rate': 0.00024208352248131275, 'dropout_rate': 0.1720721051812883, 'bits': 1603, 'radius': 1, 'use_counts': False}\n",
      "1.3315160651098596\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names:\n",
    "    \n",
    "    best_error = float('inf')\n",
    "    \n",
    "    for i in range(NUM_SEARCHES):\n",
    "        \n",
    "        np.random.seed(42+i)\n",
    "        \n",
    "        num_layers = parameters['num_layers']()\n",
    "        num_units = parameters['num_units']()\n",
    "        learning_rate = parameters['learning_rate']()\n",
    "        batch_size = parameters['batch_size']()\n",
    "        num_epochs = parameters['num_epochs']()\n",
    "        dropout_rate = parameters['dropout_rate']()\n",
    "        bits = parameters['bits']()\n",
    "        radius = parameters['radius']()\n",
    "        use_counts = parameters['use_counts']()\n",
    "        \n",
    "        train, valid, test_1, test_2 = mlp_datasets.get_ecfp_datasets(\n",
    "            '../input/datasets/{}.csv'.format(dataset_name),\n",
    "            bits=bits, radius=radius, use_counts=use_counts,\n",
    "        )\n",
    "        \n",
    "        model = mlp_models.MLPModel(\n",
    "            hidden_units=[num_units] * num_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            loss_fn=tf.keras.losses.Huber,\n",
    "            optimizer=tf.keras.optimizers.Adam,\n",
    "            initial_learning_rate=learning_rate,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if test_2 is not None:\n",
    "            additional_datasets = {\n",
    "                'valid':  [valid['X'],  valid['y']],\n",
    "                'test_1': [test_1['X'], test_1['y']],\n",
    "                'test_2': [test_2['X'], test_2['y']],\n",
    "            }\n",
    "        else:\n",
    "            additional_datasets = {\n",
    "                'valid':  [valid['X'],  valid['y']],\n",
    "                'test': [test_1['X'], test_1['y']],\n",
    "            }\n",
    "        \n",
    "        model.fit(train['X'], train['y'], \n",
    "                  additional_datasets=additional_datasets,\n",
    "                  batch_size=batch_size, verbose=0,\n",
    "                  epochs=num_epochs)\n",
    "        \n",
    "        if not os.path.isdir(f'../output/learning_curves/{dataset_name}/mlp/'):\n",
    "            os.makedirs(f'../output/learning_curves/{dataset_name}/mlp/')\n",
    "                \n",
    "        for k, v in model.learning_curves.items():\n",
    "            np.save(f'../output/learning_curves/{dataset_name}/mlp/{k}_{i}.npy', \n",
    "                    np.array(list(v)))\n",
    "        \n",
    "        \n",
    "        trues, preds = model.predict(valid['X'], valid['y'])\n",
    "\n",
    "        error = metrics.get('mae')(trues, preds)\n",
    "        \n",
    "        print(error)\n",
    "        \n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_params = {\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"hidden_units\": [num_units] * num_layers,\n",
    "                \"initial_learning_rate\": learning_rate,\n",
    "                \"dropout_rate\": dropout_rate,\n",
    "                \"bits\": bits,\n",
    "                \"radius\": radius,\n",
    "                \"use_counts\": use_counts,\n",
    "            }\n",
    "            best_weights = model.get_weights()\n",
    "            \n",
    "            for k, v in model.learning_curves.items():\n",
    "                np.save(f'../output/learning_curves/{dataset_name}/mlp/{k}_best.npy', \n",
    "                        np.array(list(v)))\n",
    "\n",
    "    generate_output(\n",
    "        model_obj=mlp_models.MLPModel,\n",
    "        model_params=best_params,\n",
    "        model_weights=best_weights,\n",
    "        save_path='../output/predictions/{}/{}'.format(\n",
    "            dataset_name, model_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'num_epochs': 163, 'batch_size': 128, 'hidden_units': [677, 677], 'initial_learning_rate': 0.0010187757243303552, 'dropout_rate': 0.21726993516627638, 'bits': 1722, 'radius': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
