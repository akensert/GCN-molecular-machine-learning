{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Lipinski\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import DataStructs\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from gcn.datasets import GCNDataset\n",
    "from gcn.models import GCNModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_atom_saliency(model, A, H, y):\n",
    "\n",
    "    # remove potential padding\n",
    "    keep_idx = np.where(A.sum(axis=1) != 0)[0]\n",
    "    H = tf.convert_to_tensor(H[keep_idx])[tf.newaxis]\n",
    "    A = tf.convert_to_tensor(A[keep_idx][:, keep_idx])[tf.newaxis]\n",
    "    y = tf.convert_to_tensor(y)[tf.newaxis]\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(H)\n",
    "        y_pred = model([A, H], training=False)\n",
    "        loss = tf.compat.v1.losses.huber_loss(y, y_pred)\n",
    "    \n",
    "    gradients = tape.gradient(loss, H)\n",
    "    gradients = tf.abs(gradients)\n",
    "    return tf.reduce_sum(gradients[0], axis=1).numpy()\n",
    "\n",
    "def draw_atom_saliency_on_mol(mol, saliency, path, size=(1000, 1000)):\n",
    "\n",
    "    if not os.path.isdir('/'.join(path.split('/')[:-1])):\n",
    "        os.makedirs('/'.join(path.split('/')[:-1]))\n",
    "\n",
    "    drawer = Draw.MolDraw2DCairo(*size)\n",
    "    drawer.drawOptions().bondLineWidth = 3\n",
    "\n",
    "    saliency = saliency / saliency.max()\n",
    "    \n",
    "    Draw.SimilarityMaps.GetSimilarityMapFromWeights(\n",
    "        mol=mol,\n",
    "        weights=[float(s) for s in saliency],\n",
    "        size=size,\n",
    "        coordScale=1.0,\n",
    "        colors='g',\n",
    "        alpha=0.4,\n",
    "        contourLines=10,\n",
    "        draw2d=drawer);\n",
    "\n",
    "    drawer.FinishDrawing()\n",
    "    drawer.WriteDrawingText(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Obtain identical/similar compounds between the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hilic_dataset = GCNDataset(\n",
    "    [f'../input/tfrecords/Fiehn_HILIC/train.tfrec',\n",
    "     f'../input/tfrecords/Fiehn_HILIC/valid.tfrec',\n",
    "     f'../input/tfrecords/Fiehn_HILIC/test_1.tfrec',\n",
    "     f'../input/tfrecords/Fiehn_HILIC/test_2.tfrec'], \n",
    "    1, False).get_iterator()\n",
    "\n",
    "rplc_dataset = GCNDataset(\n",
    "    [f'../input/tfrecords/RIKEN/train.tfrec',\n",
    "     f'../input/tfrecords/RIKEN/valid.tfrec',\n",
    "     f'../input/tfrecords/RIKEN/test_1.tfrec',\n",
    "     f'../input/tfrecords/RIKEN/test_2.tfrec'], \n",
    "    1, False\n",
    ").get_iterator()\n",
    "\n",
    "\n",
    "hilic_data = []\n",
    "rplc_data = []\n",
    "\n",
    "for example_1 in tqdm(hilic_dataset):\n",
    "    for example_2 in rplc_dataset:\n",
    "\n",
    "        mol_a = example_1['string'].numpy()[0][0].decode('utf-8')\n",
    "        mol_b = example_2['string'].numpy()[0][0].decode('utf-8')\n",
    "        mol_a = Chem.MolFromSmiles(mol_a)\n",
    "        mol_b = Chem.MolFromSmiles(mol_b)\n",
    "        \n",
    "        if (mol_a.HasSubstructMatch(mol_b) and mol_b.HasSubstructMatch(mol_a)):\n",
    "            \n",
    "            A = example_1['adjacency_matrix'].numpy()[0]\n",
    "            H = example_1['feature_matrix'].numpy()[0]\n",
    "            y = example_1['label'].numpy()[0]\n",
    "            i = example_1['index'].numpy()[0][0]\n",
    "            \n",
    "            hilic_data.append((A, H, y, mol_a, i))\n",
    "            \n",
    "            A = example_2['adjacency_matrix'].numpy()[0]\n",
    "            H = example_2['feature_matrix'].numpy()[0]\n",
    "            y = example_2['label'].numpy()[0]\n",
    "            j = example_2['index'].numpy()[0][0]\n",
    "            \n",
    "            rplc_data.append((A, H, y, mol_b, j))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "\n",
    "# Fiehn HILIC\n",
    "# Train on all data\n",
    "train_dataset_hilic = GCNDataset(\n",
    "    [f'../input/tfrecords/Fiehn_HILIC/train.tfrec',\n",
    "     f'../input/tfrecords/Fiehn_HILIC/valid.tfrec',\n",
    "     f'../input/tfrecords/Fiehn_HILIC/test_1.tfrec',\n",
    "     f'../input/tfrecords/Fiehn_HILIC/test_2.tfrec'], \n",
    "    batch_size, training=True)\n",
    "\n",
    "# build model (with default hyper-parameters)\n",
    "gcn_model_hilic = GCNModel()\n",
    "\n",
    "# fit model for {num_epochs} with a batch_size of {batch_size}\n",
    "gcn_model_hilic.fit(\n",
    "    train_dataset_hilic.get_iterator(), \n",
    "    epochs=num_epochs, verbose=1\n",
    ")\n",
    "\n",
    "# RIKEN\n",
    "# Train on all data\n",
    "train_dataset_rplc = GCNDataset(\n",
    "    [f'../input/tfrecords/RIKEN/train.tfrec',\n",
    "     f'../input/tfrecords/RIKEN/valid.tfrec',\n",
    "     f'../input/tfrecords/RIKEN/test_1.tfrec',\n",
    "     f'../input/tfrecords/RIKEN/test_2.tfrec'], \n",
    "    batch_size, training=True)\n",
    "\n",
    "# build model (with default hyper-parameters)\n",
    "gcn_model_rplc = GCNModel()\n",
    "\n",
    "# fit model for {num_epochs} with a batch_size of {batch_size}\n",
    "gcn_model_rplc.fit(\n",
    "    train_dataset_rplc.get_iterator(), \n",
    "    epochs=num_epochs, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Obtain saliences for compounds obtained in (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 0\n",
    "\n",
    "for example_1, example_2 in zip(hilic_data, rplc_data):\n",
    "\n",
    "    (A, H, y, mol, i) = example_1\n",
    "\n",
    "    saliency_map = vanilla_atom_saliency(gcn_model_hilic, A, H, y)\n",
    "\n",
    "    draw_atom_saliency_on_mol(\n",
    "        mol, saliency_map, f'../output/saliency/pair-{pair}_Fiehn-HILIC-index-{i}.png')\n",
    "\n",
    "    (A, H, y, mol, i) = example_2\n",
    "\n",
    "    saliency_map = vanilla_atom_saliency(gcn_model_rplc, A, H, y)\n",
    "\n",
    "    draw_atom_saliency_on_mol(\n",
    "        mol, saliency_map, f'../output/saliency/pair-{pair}_RIKEN-index-{i}.png')\n",
    "\n",
    "    pair += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
