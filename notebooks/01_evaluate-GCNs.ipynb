{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from gcn import datasets as gcn_datasets\n",
    "from gcn import models as gcn_models\n",
    "from rgcn import datasets as rgcn_datasets\n",
    "from rgcn import models as rgcn_models\n",
    "\n",
    "from utils import metrics\n",
    "\n",
    "import configuration\n",
    "\n",
    "from notebook_utils import fit_and_predict_gnn as fit_and_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {\n",
    "    'num_gconv_layers': lambda:       np.random.randint(*[3, 5+1]),\n",
    "    'num_gconv_units':  lambda:       np.random.randint(*[128, 256+1]),\n",
    "    'learning_rate':    lambda:       np.random.uniform(*[2e-4, 2e-3]),\n",
    "    'num_epochs':       lambda:       np.random.randint(*[100, 300+1]),\n",
    "    'batch_size':       lambda:       np.random.choice([32, 64, 128]),\n",
    "    'weight_decay':     lambda:   10**np.random.uniform(*[-10, -5]),\n",
    "    'num_dense_layers': lambda:       np.random.randint(*[1, 2+1]),\n",
    "    'num_dense_units':  lambda:       np.random.randint(*[256, 1024]),\n",
    "    'dense_dropout':    lambda:       np.random.uniform(*[0.0, 0.3]),\n",
    "}\n",
    "\n",
    "\n",
    "models = {\n",
    "    'gcn': {\n",
    "        'dataset': gcn_datasets.GCNDataset,\n",
    "        'model': gcn_models.GCNModel\n",
    "    },\n",
    "    'rgcn': {\n",
    "        'dataset': rgcn_datasets.RGCNDataset,\n",
    "        'model': rgcn_models.RGCNModel\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "dataset_names = list(configuration.datasets.keys())\n",
    "\n",
    "NUM_REPL = configuration.NUM_REPLICATES\n",
    "NUM_SEARCHES = configuration.NUM_SEARCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for model_name in models.keys():\n",
    "    \n",
    "    for dataset_name in dataset_names:\n",
    "\n",
    "        best_error = float('inf')\n",
    "\n",
    "        for i in range(NUM_SEARCHES):\n",
    "\n",
    "            np.random.seed(42+i)\n",
    "\n",
    "            num_gconv_layers = parameters['num_gconv_layers']()\n",
    "            num_gconv_units = parameters['num_gconv_units']()\n",
    "            learning_rate = parameters['learning_rate']()\n",
    "            batch_size = parameters['batch_size']()\n",
    "            num_epochs = parameters['num_epochs']()\n",
    "            weight_decay = parameters['weight_decay']()\n",
    "            num_dense_layers = parameters['num_dense_layers']()\n",
    "            num_dense_units = parameters['num_dense_units']()\n",
    "            dense_dropout = parameters['dense_dropout']()\n",
    "\n",
    "            print('Model                     : {}'.format(model_name))\n",
    "            print('Dataset                   : {}'.format(dataset_name))\n",
    "            print('Number of gconv layers    : {}'.format(num_gconv_layers))\n",
    "            print('Learning rate             : {}'.format(learning_rate))\n",
    "            print('Batch size                : {}'.format(batch_size))\n",
    "            print('Number of epochs          : {}'.format(num_epochs))\n",
    "            print('Weight decay              : {}'.format(weight_decay))\n",
    "            print('Number of dense layers    : {}'.format(num_dense_layers))\n",
    "            print('Number of dense units     : {}'.format(num_dense_units))\n",
    "            print('Dropout                   : {}'.format(dense_dropout))\n",
    "\n",
    "   \n",
    "            params = {\n",
    "                \"gconv_units\": [num_gconv_units] * num_gconv_layers,\n",
    "                \"gconv_regularizer\": tf.keras.regularizers.L2(weight_decay),\n",
    "                \"initial_learning_rate\": learning_rate,\n",
    "                'dense_units': [num_dense_units] * num_dense_layers,\n",
    "                'dense_dropout': dense_dropout,\n",
    "            }\n",
    "         \n",
    "            train_dataset = models[model_name]['dataset'](\n",
    "                f'../input/tfrecords/{dataset_name}/train.tfrec', batch_size, True)\n",
    "            valid_dataset = models[model_name]['dataset'](\n",
    "                f'../input/tfrecords/{dataset_name}/valid.tfrec', batch_size, False)\n",
    "            test_dataset = models[model_name]['dataset'](\n",
    "                f'../input/tfrecords/{dataset_name}/test.tfrec', batch_size, False)\n",
    "\n",
    "\n",
    "            model = models[model_name]['model'](**params)\n",
    "\n",
    "            model.fit(\n",
    "                train_dataset.get_iterator(), \n",
    "                epochs=num_epochs, verbose=0\n",
    "            )\n",
    "\n",
    "\n",
    "            trues, preds = model.predict(valid_dataset.get_iterator(), verbose=0)\n",
    "\n",
    "            error = metrics.get('rmse')(trues, preds)\n",
    "            print('RMSE                      : {}\\n'.format(error) + '---'*20)\n",
    "\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_params = params.copy()\n",
    "                best_weights = model.get_weights()\n",
    "                best_params['batch_size'] = batch_size\n",
    "                best_params['num_epochs'] = num_epochs\n",
    "            \n",
    "        fit_and_predict(\n",
    "            model_obj=models[model_name]['model'],\n",
    "            model_params=best_params,\n",
    "            model_weights=best_weights,\n",
    "            datasets=[train_dataset, valid_dataset, test_dataset],\n",
    "            num_repl=NUM_REPL,\n",
    "            save_path='../output/predictions/{}/{}'.format(\n",
    "                dataset_name, model_name)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
