{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from gcn.datasets import GCNDataset\n",
    "from gcn.models import GCNModel\n",
    "from utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 000 : lr 0.001000 : loss 13.125 : :   0%|          | 1/1974 [00:00<32:17,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1193 calls to <function GCNModel.call at 0x7f64c034cf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 000 : lr 0.001000 : loss 11.589 : :   0%|          | 2/1974 [00:01<24:44,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 1195 calls to <function GCNModel.call at 0x7f64c034cf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 000 : lr 0.001000 : loss 1.128 : : 100%|██████████| 1974/1974 [00:20<00:00, 98.15it/s] \n",
      "epoch 001 : lr 0.001000 : loss 0.888 : : 100%|██████████| 1974/1974 [00:19<00:00, 102.85it/s]\n",
      "epoch 002 : lr 0.001000 : loss 0.774 : : 100%|██████████| 1974/1974 [00:19<00:00, 103.44it/s]\n",
      "epoch 003 : lr 0.001000 : loss 0.702 : : 100%|██████████| 1974/1974 [00:19<00:00, 101.71it/s]\n",
      "epoch 004 : lr 0.001000 : loss 0.651 : : 100%|██████████| 1974/1974 [00:19<00:00, 103.09it/s]\n",
      "epoch 005 : lr 0.001000 : loss 0.612 : : 100%|██████████| 1974/1974 [00:18<00:00, 107.81it/s]\n",
      "epoch 006 : lr 0.001000 : loss 0.581 : : 100%|██████████| 1974/1974 [00:18<00:00, 105.46it/s]\n",
      "epoch 007 : lr 0.001000 : loss 0.555 : : 100%|██████████| 1974/1974 [00:19<00:00, 103.63it/s]\n",
      "epoch 008 : lr 0.001000 : loss 0.533 : : 100%|██████████| 1974/1974 [00:18<00:00, 106.78it/s]\n",
      "epoch 009 : lr 0.001000 : loss 0.519 : :  75%|███████▍  | 1473/1974 [00:14<00:04, 102.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-acfcc90bc8cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Increase 'epochs' for better trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Predict on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/research/finished/GCN-molecular-machine-learning/notebooks/../src/base_classes/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, additional_datasets, epochs, steps_per_epoch, verbose)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/research/finished/GCN-molecular-machine-learning/notebooks/../src/base_classes/models.py\u001b[0m in \u001b[0;36m_train_step\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    235\u001b[0m           loss_metric_values)\n\u001b[1;32m    236\u001b[0m       \u001b[0mtotal_loss_metric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_metric_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m       self._loss_metric.update_state(\n\u001b[0m\u001b[1;32m    238\u001b[0m           total_loss_metric_value, sample_weight=batch_dim)\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update_op will be None in eager execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mcontrol_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mag_update_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_update_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    348\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, values, sample_weight)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mnum_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mnum_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m       raise NotImplementedError(\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0;32m-> 2044\u001b[0;31m                               _ReductionDims(input_tensor, axis))\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_ReductionDims\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m   1905\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m       \u001b[0;31m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mrange\u001b[0;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[1;32m   1875\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/dl/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_range\u001b[0;34m(start, limit, delta, name)\u001b[0m\n\u001b[1;32m   7186\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7187\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7188\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   7189\u001b[0m         _ctx, \"Range\", name, start, limit, delta)\n\u001b[1;32m   7190\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = GCNDataset(\n",
    "    f'../input/tfrecords/SMRT/train.tfrec', 32, True)\n",
    "valid_dataset = GCNDataset(\n",
    "    f'../input/tfrecords/SMRT/valid.tfrec', 32, False)\n",
    "\n",
    "\n",
    "# build model with default parameters\n",
    "model = GCNModel()\n",
    "\n",
    "# Increase 'epochs' for better trained model\n",
    "model.fit(train_dataset.get_iterator(), epochs=32, verbose=1)\n",
    "\n",
    "# Predict on the validation set\n",
    "trues, preds = model.predict(valid_dataset.get_iterator())\n",
    "\n",
    "# Visualize the performance of the model on the validation set\n",
    "print(\"MAE =\", metrics.get('mae')(trues, preds))\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(trues, preds, alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN and RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe386f195c104d938d4c0ca165f546e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE                       : 2.201793670654297\n",
      "\n",
      "MAE                       : 2.2988686561584473\n",
      "\n",
      "MAE                       : 1.2982888221740723\n",
      "\n",
      "MAE                       : 1.4730702638626099\n",
      "\n",
      "MAE                       : 1.52862548828125\n",
      "\n",
      "MAE                       : 1.3780031204223633\n",
      "\n",
      "MAE                       : 1.4328747987747192\n",
      "\n",
      "MAE                       : 1.3098328113555908\n",
      "\n",
      "MAE                       : 1.9545739889144897\n",
      "\n",
      "MAE                       : 2.060023307800293\n",
      "\n",
      "MAE                       : 3.8330249786376953\n",
      "\n",
      "MAE                       : 2.409986734390259\n",
      "\n",
      "MAE                       : 2.2008774280548096\n",
      "\n",
      "MAE                       : 1.2929469347000122\n",
      "\n",
      "MAE                       : 1.384147047996521\n",
      "\n",
      "MAE                       : 1.3568565845489502\n",
      "\n",
      "MAE                       : 1.4860260486602783\n",
      "\n",
      "MAE                       : 1.3671960830688477\n",
      "\n",
      "MAE                       : 1.3529067039489746\n",
      "\n",
      "MAE                       : 2.3811259269714355\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0fa213d94f42afb21f42f5d7ecf7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE                       : 2.0274300575256348\n",
      "\n",
      "MAE                       : 2.4033195972442627\n",
      "\n",
      "MAE                       : 1.5186339616775513\n",
      "\n",
      "MAE                       : 1.7884743213653564\n",
      "\n",
      "MAE                       : 2.0614984035491943\n",
      "\n",
      "MAE                       : 1.4743832349777222\n",
      "\n",
      "MAE                       : 1.790137529373169\n",
      "\n",
      "MAE                       : 1.9855717420578003\n",
      "\n",
      "MAE                       : 2.2390644550323486\n",
      "\n",
      "MAE                       : 2.461198568344116\n",
      "\n",
      "MAE                       : 2.757524251937866\n",
      "\n",
      "MAE                       : 2.391857385635376\n",
      "\n",
      "MAE                       : 2.3500943183898926\n",
      "\n",
      "MAE                       : 1.5171890258789062\n",
      "\n",
      "MAE                       : 2.448542594909668\n",
      "\n",
      "MAE                       : 1.9523109197616577\n",
      "\n",
      "MAE                       : 1.7613800764083862\n",
      "\n",
      "MAE                       : 1.8731651306152344\n",
      "\n",
      "MAE                       : 1.5698970556259155\n",
      "\n",
      "MAE                       : 1.9250293970108032\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ab90a4553c48aca441eef54395e32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE                       : 1.2058038711547852\n",
      "\n",
      "MAE                       : 1.4292271137237549\n",
      "\n",
      "MAE                       : 1.0252010822296143\n",
      "\n",
      "MAE                       : 0.9820144772529602\n",
      "\n",
      "MAE                       : 1.12380051612854\n",
      "\n",
      "MAE                       : 1.0325639247894287\n",
      "\n",
      "MAE                       : 1.2132335901260376\n",
      "\n",
      "MAE                       : 1.315009355545044\n",
      "\n",
      "MAE                       : 1.2902445793151855\n",
      "\n",
      "MAE                       : 1.1912362575531006\n",
      "\n",
      "MAE                       : 1.5911176204681396\n",
      "\n",
      "MAE                       : 1.2648193836212158\n",
      "\n",
      "MAE                       : 1.2258834838867188\n",
      "\n",
      "MAE                       : 1.0232319831848145\n",
      "\n",
      "MAE                       : 1.4992321729660034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "for dataset_name in dataset_names:\n",
    "\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    for i in tqdm(range(NUM_SEARCHES)):\n",
    "\n",
    "        np.random.seed(42+i)\n",
    "\n",
    "        num_gconv_layers = parameters['num_gconv_layers']()\n",
    "        num_gconv_units = parameters['num_gconv_units']()\n",
    "        learning_rate = parameters['learning_rate']()\n",
    "        batch_size = parameters['batch_size']()\n",
    "        num_epochs = parameters['num_epochs']()\n",
    "        weight_decay = parameters['weight_decay']()\n",
    "        num_dense_layers = parameters['num_dense_layers']()\n",
    "        num_dense_units = parameters['num_dense_units']()\n",
    "        dense_dropout = parameters['dense_dropout']()\n",
    "        \n",
    "        params = {\n",
    "            \"gconv_units\": [num_gconv_units] * num_gconv_layers,\n",
    "            \"gconv_regularizer\": tf.keras.regularizers.L2(weight_decay),\n",
    "            \"initial_learning_rate\": learning_rate,\n",
    "            'dense_units': [num_dense_units] * num_dense_layers,\n",
    "            'dense_dropout': dense_dropout,\n",
    "        }\n",
    "        \n",
    "        train_dataset = gcn_datasets.GCNDataset(\n",
    "            f'../input/tfrecords/{dataset_name}/train.tfrec', batch_size, True)\n",
    "        valid_dataset = gcn_datasets.GCNDataset(\n",
    "            f'../input/tfrecords/{dataset_name}/valid.tfrec', batch_size, False)\n",
    "        \n",
    "        model = gcn_models.GCNModel(**params)\n",
    "        model.fit(\n",
    "            train_dataset.get_iterator(), \n",
    "            epochs=num_epochs, \n",
    "            verbose=0\n",
    "        )\n",
    "            \n",
    "        trues, preds = model.predict(valid_dataset.get_iterator())\n",
    "\n",
    "        error = metrics.get('mae')(trues, preds)\n",
    "        print(f'MAE                       : {error}\\n')\n",
    "\n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_params = params.copy()\n",
    "            best_weights = model.get_weights()\n",
    "            \n",
    "    generate_output(\n",
    "        model_obj=gcn_models.GCNModel,\n",
    "        model_params=best_params,\n",
    "        model_weights=best_weights,\n",
    "        dataset_obj=gcn_datasets.GCNDataset,\n",
    "        save_path=f'../output/predictions/{dataset_name}/gcn/'\n",
    "    )\n",
    "\n",
    "    \n",
    "for dataset_name in dataset_names:\n",
    "\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    for i in tqdm(range(NUM_SEARCHES)):\n",
    "\n",
    "        np.random.seed(42+i)\n",
    "        \n",
    "        num_bases = parameters['num_bases']()\n",
    "        num_gconv_layers = parameters['num_gconv_layers']()\n",
    "        num_gconv_units = parameters['num_gconv_units']()\n",
    "        learning_rate = parameters['learning_rate']()\n",
    "        batch_size = parameters['batch_size']()\n",
    "        num_epochs = parameters['num_epochs']()\n",
    "        weight_decay = parameters['weight_decay']()\n",
    "        num_dense_layers = parameters['num_dense_layers']()\n",
    "        num_dense_units = parameters['num_dense_units']()\n",
    "        dense_dropout = parameters['dense_dropout']()\n",
    "        \n",
    "        \n",
    "        params = {\n",
    "           # \"gconv_num_bases\": num_bases,\n",
    "            \"gconv_units\": [num_gconv_units] * num_gconv_layers,\n",
    "            \"gconv_regularizer\": tf.keras.regularizers.L2(weight_decay),\n",
    "            \"initial_learning_rate\": learning_rate,\n",
    "            'dense_units': [num_dense_units] * num_dense_layers,\n",
    "            'dense_dropout': dense_dropout,\n",
    "        }\n",
    "\n",
    "        train_dataset = rgcn_datasets.RGCNDataset(\n",
    "            f'../input/tfrecords/{dataset_name}/train.tfrec', batch_size, True)\n",
    "        valid_dataset = rgcn_datasets.RGCNDataset(\n",
    "            f'../input/tfrecords/{dataset_name}/valid.tfrec', batch_size, False)\n",
    "            \n",
    "        model = rgcn_models.RGCNModel(**params)\n",
    "        model.fit(\n",
    "            train_dataset.get_iterator(), \n",
    "            epochs=num_epochs, \n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "\n",
    "        trues, preds = model.predict(valid_dataset.get_iterator(), verbose=0)\n",
    "\n",
    "        error = metrics.get('mae')(trues, preds)\n",
    "        \n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_params = params.copy()\n",
    "            best_weights = model.get_weights()\n",
    "    \n",
    "    generate_output(\n",
    "        model_obj=rgcn_models.RGCNModel,\n",
    "        model_params=best_params,\n",
    "        model_weights=best_weights,\n",
    "        dataset_obj=rgcn_datasets.RGCNDataset,\n",
    "        save_path=f'../output/predictions/{dataset_name}/rgcn/'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model_obj,\n",
    "                    model_params,\n",
    "                    model_weights,\n",
    "                    save_path):\n",
    "\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    batch_size = model_params['batch_size']\n",
    "    num_epochs = model_params['num_epochs']\n",
    "    bits = model_params['bits']\n",
    "    radius = model_params['radius']\n",
    "    use_counts = model_params['use_counts']\n",
    "\n",
    "    del model_params['batch_size']\n",
    "    del model_params['num_epochs']\n",
    "    del model_params['bits']\n",
    "    del model_params['radius']\n",
    "    del model_params['use_counts']\n",
    "    \n",
    "    train, valid, test_1, test_2 = mlp_datasets.get_ecfp_datasets(\n",
    "        f\"../input/datasets/{save_path.split('/')[-2]}.csv\",\n",
    "        bits=bits, radius=radius, use_counts=use_counts,\n",
    "    )\n",
    "    \n",
    "    model = model_obj(**model_params)\n",
    "    model(train['X'][:1])\n",
    "    model.set_weights(model_weights)\n",
    "    \n",
    "    for name, dataset in zip(['train', 'valid', 'test_1', 'test_2'], [train, valid, test_1, test_2]):\n",
    "        if dataset is not None:\n",
    "            y_pred = model.predict(dataset['X'], dataset['y'])[1]\n",
    "            np.save(save_path + '/' + name, np.stack([dataset['y'], y_pred]))\n",
    "    \n",
    "parameters = {\n",
    "    'num_layers':    lambda:       np.random.randint(*[1, 3+1]),\n",
    "    'num_units':     lambda:       np.random.randint(*[256, 1024+1]),\n",
    "    'learning_rate': lambda:       np.random.uniform(*[1e-4, 1e-3]),\n",
    "    'num_epochs':    lambda:       np.random.randint(*[1, 2+1]),\n",
    "    'batch_size':    lambda:       np.random.choice([32, 64, 128]),\n",
    "    'dropout_rate':  lambda:       np.random.uniform(*[0.0, 0.3]),\n",
    "    'bits':          lambda:       np.random.randint(*[512, 2048+1]),\n",
    "    'radius':        lambda:       np.random.randint(*[1, 3+1]),\n",
    "    'use_counts':    lambda:       np.random.choice([True, False]),\n",
    "}\n",
    "\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    \n",
    "    best_error = float('inf')\n",
    "    \n",
    "    for i in tqdm(range(NUM_SEARCHES)):\n",
    "        \n",
    "        np.random.seed(42+i)\n",
    "        \n",
    "        num_layers = parameters['num_layers']()\n",
    "        num_units = parameters['num_units']()\n",
    "        learning_rate = parameters['learning_rate']()\n",
    "        batch_size = parameters['batch_size']()\n",
    "        num_epochs = parameters['num_epochs']()\n",
    "        dropout_rate = parameters['dropout_rate']()\n",
    "        bits = parameters['bits']()\n",
    "        radius = parameters['radius']()\n",
    "        use_counts = parameters['use_counts']()\n",
    "        \n",
    "        train, valid, test_1, test_2 = mlp_datasets.get_ecfp_datasets(\n",
    "            '../input/datasets/{}.csv'.format(dataset_name),\n",
    "            bits=bits, radius=radius, use_counts=use_counts,\n",
    "        )\n",
    "        \n",
    "        model = mlp_models.MLPModel(\n",
    "            hidden_units=[num_units] * num_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            loss_fn=tf.keras.losses.Huber,\n",
    "            optimizer=tf.keras.optimizers.Adam,\n",
    "            initial_learning_rate=learning_rate,\n",
    "        )\n",
    "        \n",
    "        model.fit(train['X'], train['y'], \n",
    "                  batch_size=batch_size, \n",
    "                  verbose=0,\n",
    "                  epochs=num_epochs)\n",
    "        \n",
    "        trues, preds = model.predict(valid['X'], valid['y'])\n",
    "\n",
    "        error = metrics.get('mae')(trues, preds)\n",
    "        \n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_params = {\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"hidden_units\": [num_units] * num_layers,\n",
    "                \"initial_learning_rate\": learning_rate,\n",
    "                \"dropout_rate\": dropout_rate,\n",
    "                \"bits\": bits,\n",
    "                \"radius\": radius,\n",
    "                \"use_counts\": use_counts,\n",
    "            }\n",
    "            best_weights = model.get_weights()\n",
    "            \n",
    "\n",
    "    generate_output(\n",
    "        model_obj=mlp_models.MLPModel,\n",
    "        model_params=best_params,\n",
    "        model_weights=best_weights,\n",
    "        save_path='../output/predictions/{}/{}'.format(dataset_name, 'mlp')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model_obj, datasets, save_path):\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    for name, dataset in zip(['train', 'valid', 'test_1', 'test_2'], datasets):\n",
    "        if dataset is not None:\n",
    "            y_pred = model_obj.predict(dataset['X'])\n",
    "            np.save(save_path + '/' + name, np.stack([dataset['y'], y_pred]))\n",
    "    \n",
    "model_names = [\n",
    "    'rf', \n",
    "    'gb',\n",
    "    'ab',\n",
    "    'svm',\n",
    "]\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    for dataset_name in dataset_names:\n",
    "        \n",
    "        train, valid, test_1, test_2 = ml_datasets.get_descriptor_datasets(\n",
    "            dataset_path=f'../input/datasets/{dataset_name}.csv')\n",
    "        \n",
    "        model_iter = ml_models.ModelGenerator(model_name, NUM_SEARCHES)\n",
    "        \n",
    "        best_error = float('inf')\n",
    "        for model in tqdm(model_iter):\n",
    "            \n",
    "            model.fit(train['X'], train['y'])\n",
    "            preds = model.predict(valid['X'])\n",
    "            error = metrics.get('mae')(valid['y'], preds)\n",
    "            \n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_model = model\n",
    "        \n",
    "        print('model      : {}'.format(model_name))\n",
    "        print('dataset    : {}'.format(dataset_name))\n",
    "        print('best score : {}'.format(best_error))\n",
    "        print('---'*20)\n",
    "        \n",
    "        generate_output(\n",
    "            model_obj=best_model,\n",
    "            datasets=[train, valid, test_1, test_2],\n",
    "            save_path='../output/predictions/{}/{}'.format(\n",
    "                dataset_name, model_name)\n",
    "        )\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
